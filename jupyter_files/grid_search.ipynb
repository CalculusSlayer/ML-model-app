{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b669b995",
   "metadata": {},
   "source": [
    "This is our grid search document! Note that there are some keyboard interrupts in our output, because we ran everything again just to make sure it works prior to our final submission! Because the full search takes a long time, we interrupted the kernel. The code works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc6a5a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.framework.errors_impl import InvalidArgumentError\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc05c819",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train - [[0.53912913 0.69460631 0.5541304  1.        ]\n",
      " [0.69474787 0.72926939 0.71293327 0.06651257]\n",
      " [0.72108335 0.72747647 0.6717056  0.09779414]\n",
      " ...\n",
      " [0.40011971 0.40475123 0.4025042  0.02255257]\n",
      " [0.40026934 0.39489018 0.38616583 0.04220904]\n",
      " [0.36465659 0.36261766 0.36784242 0.02943177]]\n",
      "**************************************************\n",
      "X_test - [[0.36555439 0.37950097 0.37196519 0.01550283]\n",
      " [0.37827323 0.38936202 0.38097419 0.01553931]\n",
      " [0.37572946 0.37546691 0.37883646 0.00991563]\n",
      " ...\n",
      " [0.05132426 0.05079934 0.05481753 0.01289286]\n",
      " [0.05656142 0.05259226 0.05527561 0.01735301]\n",
      " [0.04952865 0.06618856 0.05329058 0.0570686 ]]\n",
      "**************************************************\n",
      "Y_train - [[0.69701067]\n",
      " [0.71894247]\n",
      " [0.66155926]\n",
      " ...\n",
      " [0.39762656]\n",
      " [0.37990086]\n",
      " [0.36472886]]\n",
      "**************************************************\n",
      "Y_test - [[3.80501728e-01]\n",
      " [3.78248460e-01]\n",
      " [3.72239748e-01]\n",
      " [3.77196936e-01]\n",
      " [3.80501728e-01]\n",
      " [3.78098242e-01]\n",
      " [3.78098242e-01]\n",
      " [3.71188223e-01]\n",
      " [3.73291272e-01]\n",
      " [3.72990837e-01]\n",
      " [3.67733213e-01]\n",
      " [3.84106955e-01]\n",
      " [3.89064143e-01]\n",
      " [3.91768064e-01]\n",
      " [3.91317410e-01]\n",
      " [4.18957488e-01]\n",
      " [4.23764458e-01]\n",
      " [4.23313805e-01]\n",
      " [4.25116419e-01]\n",
      " [4.35481448e-01]\n",
      " [4.30974914e-01]\n",
      " [4.23914676e-01]\n",
      " [4.20459667e-01]\n",
      " [4.25416854e-01]\n",
      " [4.26017726e-01]\n",
      " [4.25266637e-01]\n",
      " [4.23013369e-01]\n",
      " [4.15352261e-01]\n",
      " [4.16253568e-01]\n",
      " [3.95823945e-01]\n",
      " [3.92669371e-01]\n",
      " [3.95223073e-01]\n",
      " [3.94321767e-01]\n",
      " [3.74192579e-01]\n",
      " [3.65329728e-01]\n",
      " [3.77347153e-01]\n",
      " [3.82003906e-01]\n",
      " [3.80501728e-01]\n",
      " [3.92669371e-01]\n",
      " [4.01382004e-01]\n",
      " [4.03635271e-01]\n",
      " [4.01231786e-01]\n",
      " [4.21060538e-01]\n",
      " [4.21060538e-01]\n",
      " [4.27820339e-01]\n",
      " [4.29472735e-01]\n",
      " [4.51704972e-01]\n",
      " [4.59215863e-01]\n",
      " [4.76490912e-01]\n",
      " [4.65675229e-01]\n",
      " [4.46898002e-01]\n",
      " [4.45245606e-01]\n",
      " [4.39987983e-01]\n",
      " [4.35781884e-01]\n",
      " [4.34429923e-01]\n",
      " [4.34880577e-01]\n",
      " [4.50052576e-01]\n",
      " [4.40138200e-01]\n",
      " [4.50052576e-01]\n",
      " [4.31726003e-01]\n",
      " [4.39236893e-01]\n",
      " [4.44043864e-01]\n",
      " [4.49902358e-01]\n",
      " [4.47198438e-01]\n",
      " [4.30374042e-01]\n",
      " [4.16704221e-01]\n",
      " [4.27069250e-01]\n",
      " [4.21811627e-01]\n",
      " [4.22262280e-01]\n",
      " [3.94622202e-01]\n",
      " [3.66080817e-01]\n",
      " [3.57668619e-01]\n",
      " [3.68934956e-01]\n",
      " [3.73591708e-01]\n",
      " [3.62625807e-01]\n",
      " [3.60072105e-01]\n",
      " [3.67282560e-01]\n",
      " [3.58870362e-01]\n",
      " [3.29728106e-01]\n",
      " [3.34535076e-01]\n",
      " [3.33032898e-01]\n",
      " [3.22968304e-01]\n",
      " [3.19363076e-01]\n",
      " [2.95929097e-01]\n",
      " [3.00736067e-01]\n",
      " [2.84963197e-01]\n",
      " [2.67387712e-01]\n",
      " [2.82109058e-01]\n",
      " [2.76100346e-01]\n",
      " [2.60327475e-01]\n",
      " [2.70542286e-01]\n",
      " [2.84061890e-01]\n",
      " [2.87066246e-01]\n",
      " [3.04341295e-01]\n",
      " [2.91572781e-01]\n",
      " [2.80006009e-01]\n",
      " [2.70241851e-01]\n",
      " [2.74598167e-01]\n",
      " [2.65585098e-01]\n",
      " [2.54619198e-01]\n",
      " [2.35090882e-01]\n",
      " [2.11206249e-01]\n",
      " [2.32687397e-01]\n",
      " [2.34940664e-01]\n",
      " [2.44254169e-01]\n",
      " [2.23824546e-01]\n",
      " [2.35691753e-01]\n",
      " [2.22923239e-01]\n",
      " [2.11807120e-01]\n",
      " [2.15111912e-01]\n",
      " [2.53267237e-01]\n",
      " [2.64533574e-01]\n",
      " [2.66786841e-01]\n",
      " [2.58825297e-01]\n",
      " [2.61829653e-01]\n",
      " [2.63331831e-01]\n",
      " [2.70392068e-01]\n",
      " [2.61529217e-01]\n",
      " [2.45756347e-01]\n",
      " [2.49061139e-01]\n",
      " [2.68889890e-01]\n",
      " [2.76851435e-01]\n",
      " [2.63181613e-01]\n",
      " [2.46507436e-01]\n",
      " [2.78654048e-01]\n",
      " [2.66786841e-01]\n",
      " [2.68289019e-01]\n",
      " [2.51013970e-01]\n",
      " [2.28931951e-01]\n",
      " [2.26077813e-01]\n",
      " [2.21871714e-01]\n",
      " [2.24725852e-01]\n",
      " [2.22322367e-01]\n",
      " [2.35841971e-01]\n",
      " [2.38996545e-01]\n",
      " [2.42151119e-01]\n",
      " [2.52365931e-01]\n",
      " [2.66937059e-01]\n",
      " [2.86615593e-01]\n",
      " [2.95178008e-01]\n",
      " [2.95178008e-01]\n",
      " [2.84362325e-01]\n",
      " [2.86315157e-01]\n",
      " [2.77302088e-01]\n",
      " [2.98032147e-01]\n",
      " [2.87516900e-01]\n",
      " [2.91422563e-01]\n",
      " [2.90070602e-01]\n",
      " [3.13805017e-01]\n",
      " [3.02238245e-01]\n",
      " [3.05993691e-01]\n",
      " [3.06294126e-01]\n",
      " [2.96529968e-01]\n",
      " [2.98332582e-01]\n",
      " [2.90521256e-01]\n",
      " [2.75799910e-01]\n",
      " [2.72194682e-01]\n",
      " [2.69640979e-01]\n",
      " [2.67537930e-01]\n",
      " [2.77902959e-01]\n",
      " [2.58524861e-01]\n",
      " [2.65735316e-01]\n",
      " [2.70842722e-01]\n",
      " [2.61979871e-01]\n",
      " [2.71744029e-01]\n",
      " [2.86315157e-01]\n",
      " [2.84512543e-01]\n",
      " [2.86315157e-01]\n",
      " [2.85564068e-01]\n",
      " [2.77001652e-01]\n",
      " [2.57923990e-01]\n",
      " [2.44704822e-01]\n",
      " [2.43653297e-01]\n",
      " [2.30283912e-01]\n",
      " [2.30133694e-01]\n",
      " [2.39146763e-01]\n",
      " [2.57473336e-01]\n",
      " [2.46507436e-01]\n",
      " [2.57323119e-01]\n",
      " [2.61979871e-01]\n",
      " [2.59426168e-01]\n",
      " [2.71143158e-01]\n",
      " [2.60477693e-01]\n",
      " [2.65585098e-01]\n",
      " [2.60627910e-01]\n",
      " [2.79254920e-01]\n",
      " [2.86164939e-01]\n",
      " [3.02839117e-01]\n",
      " [3.04641731e-01]\n",
      " [3.02088028e-01]\n",
      " [3.04341295e-01]\n",
      " [2.85864504e-01]\n",
      " [2.85864504e-01]\n",
      " [2.69340544e-01]\n",
      " [2.62730960e-01]\n",
      " [2.84662761e-01]\n",
      " [3.03439988e-01]\n",
      " [3.07646087e-01]\n",
      " [3.06143909e-01]\n",
      " [2.99534325e-01]\n",
      " [2.92474087e-01]\n",
      " [2.72194682e-01]\n",
      " [2.75199039e-01]\n",
      " [2.44404386e-01]\n",
      " [2.40949377e-01]\n",
      " [2.50563317e-01]\n",
      " [2.48760703e-01]\n",
      " [2.36142406e-01]\n",
      " [2.41850683e-01]\n",
      " [2.32537179e-01]\n",
      " [2.13759952e-01]\n",
      " [1.89124230e-01]\n",
      " [2.04596665e-01]\n",
      " [2.16163437e-01]\n",
      " [2.06399279e-01]\n",
      " [2.08502328e-01]\n",
      " [2.23824546e-01]\n",
      " [2.05197536e-01]\n",
      " [2.06699715e-01]\n",
      " [2.09553853e-01]\n",
      " [2.02042962e-01]\n",
      " [1.92879676e-01]\n",
      " [1.72149617e-01]\n",
      " [1.84167042e-01]\n",
      " [2.01892744e-01]\n",
      " [2.06699715e-01]\n",
      " [1.93180111e-01]\n",
      " [1.93931200e-01]\n",
      " [1.90025537e-01]\n",
      " [1.90926844e-01]\n",
      " [1.73802013e-01]\n",
      " [1.76656151e-01]\n",
      " [2.03244705e-01]\n",
      " [2.02944269e-01]\n",
      " [2.00090131e-01]\n",
      " [1.74853538e-01]\n",
      " [1.42406489e-01]\n",
      " [1.72600270e-01]\n",
      " [1.32191678e-01]\n",
      " [1.30839718e-01]\n",
      " [1.00195283e-01]\n",
      " [1.06203996e-01]\n",
      " [1.03650293e-01]\n",
      " [3.90566321e-02]\n",
      " [6.68469280e-02]\n",
      " [5.19753643e-02]\n",
      " [5.19753643e-02]\n",
      " [4.19107706e-02]\n",
      " [3.81553252e-02]\n",
      " [6.35421361e-02]\n",
      " [6.78984528e-02]\n",
      " [8.86285113e-02]\n",
      " [8.68258976e-02]\n",
      " [8.17184918e-02]\n",
      " [8.89289470e-02]\n",
      " [8.05167493e-02]\n",
      " [9.08817786e-02]\n",
      " [1.06203996e-01]\n",
      " [1.09058134e-01]\n",
      " [1.10710530e-01]\n",
      " [1.14766411e-01]\n",
      " [1.01096590e-01]\n",
      " [8.54739372e-02]\n",
      " [9.05813429e-02]\n",
      " [9.11822142e-02]\n",
      " [7.94652246e-02]\n",
      " [8.27700165e-02]\n",
      " [7.46582545e-02]\n",
      " [8.80276401e-02]\n",
      " [1.14015322e-01]\n",
      " [9.53883131e-02]\n",
      " [8.24695809e-02]\n",
      " [6.97010665e-02]\n",
      " [6.08382154e-02]\n",
      " [5.54303740e-02]\n",
      " [5.37779781e-02]\n",
      " [4.89710080e-02]\n",
      " [6.02373441e-02]\n",
      " [5.43788493e-02]\n",
      " [5.72329878e-02]\n",
      " [3.34985729e-02]\n",
      " [3.92068499e-02]\n",
      " [3.81553252e-02]\n",
      " [4.19107706e-02]\n",
      " [4.40138200e-02]\n",
      " [5.01727505e-02]\n",
      " [6.08382154e-02]\n",
      " [7.01517200e-02]\n",
      " [9.23839567e-02]\n",
      " [1.14165540e-01]\n",
      " [9.86931050e-02]\n",
      " [9.35856993e-02]\n",
      " [8.92293826e-02]\n",
      " [1.03049422e-01]\n",
      " [1.08607481e-01]\n",
      " [1.01397026e-01]\n",
      " [9.05813429e-02]\n",
      " [8.15682740e-02]\n",
      " [7.24049872e-02]\n",
      " [6.47438786e-02]\n",
      " [6.18897401e-02]\n",
      " [5.67823344e-02]\n",
      " [8.12678384e-02]\n",
      " [8.02163137e-02]\n",
      " [9.94441941e-02]\n",
      " [1.00045065e-01]\n",
      " [9.31350458e-02]\n",
      " [8.96800361e-02]\n",
      " [9.47874418e-02]\n",
      " [7.30058585e-02]\n",
      " [6.69971459e-02]\n",
      " [7.00015022e-02]\n",
      " [6.92504131e-02]\n",
      " [7.42076010e-02]\n",
      " [7.60102148e-02]\n",
      " [6.69971459e-02]\n",
      " [6.69971459e-02]\n",
      " [5.96364729e-02]\n",
      " [6.20399579e-02]\n",
      " [5.99369085e-02]\n",
      " [5.31771068e-02]\n",
      " [5.24260177e-02]\n",
      " [4.68679585e-02]\n",
      " [6.02373441e-02]\n",
      " [6.74477993e-02]\n",
      " [6.77482349e-02]\n",
      " [7.07525913e-02]\n",
      " [6.87997597e-02]\n",
      " [6.32417005e-02]\n",
      " [5.82845125e-02]\n",
      " [5.97866907e-02]\n",
      " [6.20399579e-02]\n",
      " [5.42286315e-02]\n",
      " [5.61814631e-02]\n",
      " [5.36277603e-02]\n",
      " [5.46792850e-02]\n",
      " [5.37779781e-02]\n",
      " [5.49797206e-02]\n",
      " [5.76836413e-02]\n",
      " [5.61814631e-02]\n",
      " [5.87351660e-02]\n",
      " [5.45290671e-02]\n",
      " [5.52801562e-02]\n",
      " [6.39927895e-02]\n",
      " [7.64608683e-02]\n",
      " [7.84136999e-02]\n",
      " [7.37569476e-02]\n",
      " [8.39717591e-02]\n",
      " [8.69761154e-02]\n",
      " [8.78774223e-02]\n",
      " [8.98302539e-02]\n",
      " [9.43367883e-02]\n",
      " [9.01306895e-02]\n",
      " [8.56241550e-02]\n",
      " [7.15036803e-02]\n",
      " [6.87997597e-02]\n",
      " [7.73621752e-02]\n",
      " [7.55595614e-02]\n",
      " [6.57954033e-02]\n",
      " [6.42932252e-02]\n",
      " [5.82845125e-02]\n",
      " [4.92714436e-02]\n",
      " [4.83701367e-02]\n",
      " [4.37133844e-02]\n",
      " [4.32627310e-02]\n",
      " [4.07090281e-02]\n",
      " [3.99579390e-02]\n",
      " [4.13098994e-02]\n",
      " [3.93570677e-02]\n",
      " [3.33483551e-02]\n",
      " [2.70392068e-02]\n",
      " [3.15457413e-02]\n",
      " [1.54724350e-02]\n",
      " [1.96785339e-02]\n",
      " [2.83911672e-02]\n",
      " [2.13309298e-02]\n",
      " [1.63737419e-02]\n",
      " [2.01291873e-02]\n",
      " [2.35841971e-02]\n",
      " [1.87772270e-02]\n",
      " [1.17169896e-02]\n",
      " [8.26197987e-03]\n",
      " [1.30689500e-02]\n",
      " [7.96154424e-03]\n",
      " [9.91437584e-03]\n",
      " [0.00000000e+00]\n",
      " [1.50217816e-04]\n",
      " [2.40348505e-03]\n",
      " [1.24680787e-02]\n",
      " [9.31350458e-03]\n",
      " [1.36698212e-02]\n",
      " [1.57728707e-02]\n",
      " [2.14811477e-02]\n",
      " [1.20174253e-02]\n",
      " [1.02148115e-02]\n",
      " [6.45936608e-03]\n",
      " [9.01306895e-04]\n",
      " [1.09659006e-02]\n",
      " [1.54724350e-02]\n",
      " [1.78759201e-02]\n",
      " [1.27685143e-02]\n",
      " [2.79405137e-02]\n",
      " [3.37990086e-02]\n",
      " [3.36487907e-02]\n",
      " [4.28120775e-02]\n",
      " [4.59666516e-02]\n",
      " [3.47003155e-02]\n",
      " [4.61168695e-02]\n",
      " [4.97220970e-02]\n",
      " [5.45290671e-02]\n",
      " [5.31771068e-02]\n",
      " [3.45500976e-02]\n",
      " [3.27474839e-02]\n",
      " [3.66531471e-02]\n",
      " [3.81553252e-02]\n",
      " [4.02583746e-02]\n",
      " [4.35631666e-02]\n",
      " [4.50653447e-02]\n",
      " [4.76190476e-02]\n",
      " [5.76836413e-02]\n",
      " [6.87997597e-02]\n",
      " [6.83491062e-02]\n",
      " [7.88643533e-02]\n",
      " [7.12032447e-02]\n",
      " [7.25552050e-02]\n",
      " [7.30058585e-02]\n",
      " [8.32206700e-02]\n",
      " [9.85428872e-02]\n",
      " [9.77917981e-02]\n",
      " [1.05002253e-01]\n",
      " [9.88433228e-02]\n",
      " [9.35856993e-02]\n",
      " [6.72975815e-02]\n",
      " [7.49586901e-02]\n",
      " [7.36067298e-02]\n",
      " [7.72119573e-02]\n",
      " [8.14180562e-02]\n",
      " [7.37569476e-02]\n",
      " [7.52591257e-02]\n",
      " [7.48084723e-02]\n",
      " [7.45080367e-02]\n",
      " [8.02163137e-02]\n",
      " [7.39071654e-02]\n",
      " [7.37569476e-02]\n",
      " [7.79630464e-02]\n",
      " [8.75769866e-02]\n",
      " [9.35856993e-02]\n",
      " [1.08457263e-01]\n",
      " [1.05903560e-01]\n",
      " [1.08457263e-01]\n",
      " [9.82424516e-02]\n",
      " [8.45726303e-02]\n",
      " [8.87787292e-02]\n",
      " [9.29848280e-02]\n",
      " [8.05167493e-02]\n",
      " [6.80486706e-02]\n",
      " [6.39927895e-02]\n",
      " [5.58810275e-02]\n",
      " [5.30268890e-02]\n",
      " [4.61168695e-02]\n",
      " [5.54303740e-02]\n",
      " [6.21901758e-02]\n",
      " [8.02163137e-02]\n",
      " [9.92939763e-02]\n",
      " [9.10319964e-02]\n",
      " [9.31350458e-02]\n",
      " [8.62250263e-02]\n",
      " [8.33708878e-02]\n",
      " [7.99158780e-02]\n",
      " [7.00015022e-02]\n",
      " [6.51945321e-02]\n",
      " [6.97010665e-02]\n",
      " [7.40573832e-02]\n",
      " [8.83280757e-02]\n",
      " [8.62250263e-02]\n",
      " [7.72119573e-02]\n",
      " [7.91647889e-02]\n",
      " [1.01697461e-01]\n",
      " [1.36848430e-01]\n",
      " [1.25131441e-01]\n",
      " [1.31590807e-01]\n",
      " [1.28736668e-01]\n",
      " [1.33243203e-01]\n",
      " [1.24680787e-01]\n",
      " [1.21826649e-01]\n",
      " [1.27985579e-01]\n",
      " [1.23629262e-01]\n",
      " [1.21375995e-01]\n",
      " [1.20024035e-01]\n",
      " [1.09208352e-01]\n",
      " [1.05603125e-01]\n",
      " [8.80276401e-02]\n",
      " [7.96154424e-02]\n",
      " [7.73621752e-02]\n",
      " [7.54093435e-02]\n",
      " [7.42076010e-02]\n",
      " [6.53447499e-02]\n",
      " [5.27264534e-02]\n",
      " [5.72329878e-02]\n",
      " [6.72975815e-02]\n",
      " [5.94862551e-02]\n",
      " [6.48940964e-02]\n",
      " [5.78338591e-02]\n",
      " [5.93360373e-02]\n",
      " [6.21901758e-02]\n",
      " [6.33919183e-02]\n",
      " [5.69325522e-02]\n",
      " [4.25116419e-02]\n",
      " [5.22757999e-02]\n",
      " [5.31771068e-02]\n",
      " [5.19753643e-02]\n",
      " [4.22112062e-02]\n",
      " [4.19107706e-02]\n",
      " [4.35631666e-02]\n",
      " [4.68679585e-02]\n",
      " [4.92714436e-02]\n",
      " [5.09238396e-02]\n",
      " [5.39281959e-02]\n",
      " [5.25762355e-02]\n",
      " [6.75980171e-02]]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing + Training\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"../backend/Datasets/COALINDIA.csv\")\n",
    "\n",
    "# Feature selection and preprocessing\n",
    "dropped_features = ['Date', 'Symbol', 'Series', \n",
    "                    'Trades', 'Turnover', 'Deliverable Volume', \n",
    "                    '%Deliverble', 'Last', 'VWAP', 'Prev Close']\n",
    "df.drop(dropped_features, axis=1, inplace=True)\n",
    "\n",
    "# Define features (X) and target (Y)\n",
    "X = df.drop('Close', axis=1)\n",
    "Y = df['Close']\n",
    "\n",
    "# Scale the data\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_Y = MinMaxScaler()\n",
    "X = scaler_X.fit_transform(X.values)\n",
    "Y = scaler_Y.fit_transform(Y.values.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "print(f\"X_train - {X_train}\")\n",
    "print(\"*\" * 50)\n",
    "print(f\"X_test - {X_test}\")\n",
    "print(\"*\" * 50)\n",
    "\n",
    "print(f\"Y_train - {Y_train}\")\n",
    "print(\"*\" * 50)\n",
    "\n",
    "print(f\"Y_test - {Y_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e36381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_A(activation='relu', epochs=100, batch_size=32, op_learning_rate=1e-5, hidden=128, dropout=0.15):\n",
    "    # Build a neural network model with 2 hidden layers\n",
    "    # You can experiment with different architectures, including the number of layers and neurons.\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X_train.shape[1], activation=activation))\n",
    "    model.add(Dense(hidden, activation=activation))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=op_learning_rate)  # Experiment with learning rate\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, Y_test), verbose=0)\n",
    "\n",
    "    # Make predictions\n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    # Inverse transform the scaled values\n",
    "    Y_test_original = scaler_Y.inverse_transform(Y_test)\n",
    "    Y_pred_original = scaler_Y.inverse_transform(Y_pred)\n",
    "\n",
    "    print(f\"Activation: {activation}\\nEpoch: {epochs}\\nBatch Size: {batch_size}\\nOptimizer learning rate: {op_learning_rate}\")\n",
    "    print(f\"Hidden: {hidden}\\nDropout: {dropout}\")\n",
    "    # Calculate MSE and R2\n",
    "    mse = mean_squared_error(Y_test_original, Y_pred_original)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    metric = tf.keras.metrics.R2Score()\n",
    "    metric.update_state(Y_test_original, Y_pred_original)\n",
    "    r2 = metric.result().numpy()\n",
    "    print(\"R^2:\", metric.result().numpy())\n",
    "    print(\"-\"*64)\n",
    "    return mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd050e4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 508us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: relu\n",
      "Epoch: 50\n",
      "Batch Size: 24\n",
      "Optimizer learning rate: 1e-05\n",
      "Hidden: 32\n",
      "Dropout: 0.05\n",
      "Mean Squared Error: 872.1474942000863\n",
      "R^2: 0.55342925\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 489us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: relu\n",
      "Epoch: 50\n",
      "Batch Size: 24\n",
      "Optimizer learning rate: 1e-05\n",
      "Hidden: 32\n",
      "Dropout: 0.1\n",
      "Mean Squared Error: 364.55312042229264\n",
      "R^2: 0.8133358\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 483us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: relu\n",
      "Epoch: 50\n",
      "Batch Size: 24\n",
      "Optimizer learning rate: 1e-05\n",
      "Hidden: 32\n",
      "Dropout: 0.15\n",
      "Mean Squared Error: 1006.1555342822251\n",
      "R^2: 0.48481238\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: relu\n",
      "Epoch: 50\n",
      "Batch Size: 24\n",
      "Optimizer learning rate: 1e-05\n",
      "Hidden: 64\n",
      "Dropout: 0.05\n",
      "Mean Squared Error: 762.7184613244058\n",
      "R^2: 0.6094609\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 499us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: relu\n",
      "Epoch: 50\n",
      "Batch Size: 24\n",
      "Optimizer learning rate: 1e-05\n",
      "Hidden: 64\n",
      "Dropout: 0.1\n",
      "Mean Squared Error: 1183.1793415675802\n",
      "R^2: 0.39416987\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 469us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: relu\n",
      "Epoch: 50\n",
      "Batch Size: 24\n",
      "Optimizer learning rate: 1e-05\n",
      "Hidden: 64\n",
      "Dropout: 0.15\n",
      "Mean Squared Error: 744.9572630184964\n",
      "R^2: 0.61855525\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 501us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: relu\n",
      "Epoch: 50\n",
      "Batch Size: 24\n",
      "Optimizer learning rate: 1e-05\n",
      "Hidden: 128\n",
      "Dropout: 0.05\n",
      "Mean Squared Error: 536.1795594208326\n",
      "R^2: 0.72545683\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 488us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: relu\n",
      "Epoch: 50\n",
      "Batch Size: 24\n",
      "Optimizer learning rate: 1e-05\n",
      "Hidden: 128\n",
      "Dropout: 0.1\n",
      "Mean Squared Error: 704.2638114187354\n",
      "R^2: 0.6393917\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 478us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: relu\n",
      "Epoch: 50\n",
      "Batch Size: 24\n",
      "Optimizer learning rate: 1e-05\n",
      "Hidden: 128\n",
      "Dropout: 0.15\n",
      "Mean Squared Error: 416.10116339352646\n",
      "R^2: 0.7869413\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 556us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: relu\n",
      "Epoch: 50\n",
      "Batch Size: 24\n",
      "Optimizer learning rate: 0.0001\n",
      "Hidden: 32\n",
      "Dropout: 0.05\n",
      "Mean Squared Error: 48.03762153916892\n",
      "R^2: 0.975403\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 496us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: relu\n",
      "Epoch: 50\n",
      "Batch Size: 24\n",
      "Optimizer learning rate: 0.0001\n",
      "Hidden: 32\n",
      "Dropout: 0.1\n",
      "Mean Squared Error: 277.5821452932378\n",
      "R^2: 0.857868\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 480us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: relu\n",
      "Epoch: 50\n",
      "Batch Size: 24\n",
      "Optimizer learning rate: 0.0001\n",
      "Hidden: 32\n",
      "Dropout: 0.15\n",
      "Mean Squared Error: 158.33800732247278\n",
      "R^2: 0.9189253\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 506us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: relu\n",
      "Epoch: 50\n",
      "Batch Size: 24\n",
      "Optimizer learning rate: 0.0001\n",
      "Hidden: 64\n",
      "Dropout: 0.05\n",
      "Mean Squared Error: 55.692836826452336\n",
      "R^2: 0.9714833\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 494us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: relu\n",
      "Epoch: 50\n",
      "Batch Size: 24\n",
      "Optimizer learning rate: 0.0001\n",
      "Hidden: 64\n",
      "Dropout: 0.1\n",
      "Mean Squared Error: 118.98761563749149\n",
      "R^2: 0.9390741\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 492us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: relu\n",
      "Epoch: 50\n",
      "Batch Size: 24\n",
      "Optimizer learning rate: 0.0001\n",
      "Hidden: 64\n",
      "Dropout: 0.15\n",
      "Mean Squared Error: 98.06414267304665\n",
      "R^2: 0.9497877\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 540us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: relu\n",
      "Epoch: 50\n",
      "Batch Size: 24\n",
      "Optimizer learning rate: 0.0001\n",
      "Hidden: 128\n",
      "Dropout: 0.05\n",
      "Mean Squared Error: 15.308770315171536\n",
      "R^2: 0.99216133\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 492us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: relu\n",
      "Epoch: 50\n",
      "Batch Size: 24\n",
      "Optimizer learning rate: 0.0001\n",
      "Hidden: 128\n",
      "Dropout: 0.1\n",
      "Mean Squared Error: 64.4709093147817\n",
      "R^2: 0.96698856\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 491us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: relu\n",
      "Epoch: 50\n",
      "Batch Size: 24\n",
      "Optimizer learning rate: 0.0001\n",
      "Hidden: 128\n",
      "Dropout: 0.15\n",
      "Mean Squared Error: 9.939917864047803\n",
      "R^2: 0.9949104\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 506us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: relu\n",
      "Epoch: 50\n",
      "Batch Size: 24\n",
      "Optimizer learning rate: 0.001\n",
      "Hidden: 32\n",
      "Dropout: 0.05\n",
      "Mean Squared Error: 7130.982607225026\n",
      "R^2: -2.6513183\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dropout \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.15\u001b[39m]:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m         mse, r2 \u001b[38;5;241m=\u001b[39m create_model_A(activation\u001b[38;5;241m=\u001b[39mactivation, epochs\u001b[38;5;241m=\u001b[39mepochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, op_learning_rate\u001b[38;5;241m=\u001b[39mop_learning_rate, hidden\u001b[38;5;241m=\u001b[39mhidden, dropout\u001b[38;5;241m=\u001b[39mdropout)\n\u001b[1;32m     10\u001b[0m         ret\u001b[38;5;241m.\u001b[39mappend([activation, epochs, batch_size, op_learning_rate, hidden, dropout, mse, r2])\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m InvalidArgumentError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m, in \u001b[0;36mcreate_model_A\u001b[0;34m(activation, epochs, batch_size, op_learning_rate, hidden, dropout)\u001b[0m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train, epochs\u001b[38;5;241m=\u001b[39mepochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, validation_data\u001b[38;5;241m=\u001b[39m(X_test, Y_test), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m     18\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    868\u001b[0m       args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_config\n\u001b[1;32m    869\u001b[0m   )\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mflat_call(args)\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:244\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    236\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    237\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSignature specifies \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m arguments, got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(args)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    238\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Expected inputs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached_definition\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39minput_arg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    239\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Received inputs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Function Type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m   )\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m InterpolateRuntimeError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 244\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcontrol_dependencies(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_options\u001b[38;5;241m.\u001b[39mcontrol_captures):\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# The caller must use record_operation to record this operation in the\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# eager case, so we enforce the same requirement for the non-eager\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;66;03m# case by explicitly pausing recording. We don't have a gradient\u001b[39;00m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# registered for PartitionedCall, so recording this operation confuses\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# forwardprop code (GradientTape manages to ignore it).\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:4492\u001b[0m, in \u001b[0;36mcontrol_dependencies\u001b[0;34m(control_inputs)\u001b[0m\n\u001b[1;32m   4430\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontrol_dependencies\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontrol_dependencies\u001b[39m(control_inputs):\n\u001b[1;32m   4432\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for `Graph.control_dependencies()` using the default graph.\u001b[39;00m\n\u001b[1;32m   4433\u001b[0m \n\u001b[1;32m   4434\u001b[0m \u001b[38;5;124;03m  See `tf.Graph.control_dependencies` for more details.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4490\u001b[0m \u001b[38;5;124;03m   operations constructed within the context.\u001b[39;00m\n\u001b[1;32m   4491\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4492\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m   4493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m control_inputs:\n\u001b[1;32m   4494\u001b[0m       \u001b[38;5;66;03m# Execute any pending callables.\u001b[39;00m\n\u001b[1;32m   4495\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m control \u001b[38;5;129;01min\u001b[39;00m control_inputs:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/context.py:2330\u001b[0m, in \u001b[0;36mexecuting_eagerly\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2328\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m default_execution_mode \u001b[38;5;241m==\u001b[39m EAGER_MODE\n\u001b[0;32m-> 2330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1012\u001b[0m, in \u001b[0;36mContext.executing_eagerly\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m EAGER_MODE:\n\u001b[1;32m   1010\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_switches\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m-> 1012\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecuting_eagerly\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns True if current thread has eager executing enabled.\"\"\"\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_thread_local_data\u001b[38;5;241m.\u001b[39mis_eager\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ret = []\n",
    "for activation in ['relu', 'sigmoid', 'tanh']:\n",
    "    for epochs in [50, 75, 100, 150]:\n",
    "        for batch_size in [24, 32, 40]:\n",
    "            for op_learning_rate in [1e-5, 1e-4, 1e-3]:\n",
    "                for hidden in [32, 64, 128]:\n",
    "                    for dropout in [0.05, 0.1, 0.15]:\n",
    "                        try:\n",
    "                            mse, r2 = create_model_A(activation=activation, epochs=epochs, batch_size=batch_size, op_learning_rate=op_learning_rate, hidden=hidden, dropout=dropout)\n",
    "                            ret.append([activation, epochs, batch_size, op_learning_rate, hidden, dropout, mse, r2])\n",
    "                        except InvalidArgumentError as e:\n",
    "                            print(\"Model error with params\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61026e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MSE for ANN: ['relu', 50, 24, 0.0001, 128, 0.15, 9.939917864047803, 0.9949104]\n",
      "Best R^2 for ANN: ['relu', 50, 24, 0.0001, 128, 0.15, 9.939917864047803, 0.9949104]\n"
     ]
    }
   ],
   "source": [
    "print(\"Best MSE for ANN:\", sorted(ret, key=lambda x:x[-2])[0])\n",
    "print(\"Best R^2 for ANN:\", sorted(ret, key=lambda x:-x[-1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04f3a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_P(activation=None, epochs=100, batch_size=32, op_learning_rate=1e-5):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=X_train.shape[1], activation=activation))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=op_learning_rate)  # Experiment with learning rate\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, Y_test), verbose=0)\n",
    "\n",
    "    # Make predictions\n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    # Inverse transform the scaled values\n",
    "    Y_test_original = scaler_Y.inverse_transform(Y_test)\n",
    "    Y_pred_original = scaler_Y.inverse_transform(Y_pred)\n",
    "\n",
    "    print(f\"Activation: {activation}\\nEpoch: {epochs}\\nBatch Size: {batch_size}\\nOptimizer learning rate: {op_learning_rate}\")\n",
    "    # Calculate MSE\n",
    "    mse = mean_squared_error(Y_test_original, Y_pred_original)\n",
    "    print(f\"Mean Squared Error (Perceptron): {mse}\")\n",
    "    metric = tf.keras.metrics.R2Score()\n",
    "    metric.update_state(Y_test_original, Y_pred_original)\n",
    "    r2 = metric.result().numpy()\n",
    "    print(\"R^2:\", metric.result().numpy())\n",
    "    print(\"-\"*64)\n",
    "    return mse, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c36e7d73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 590us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: None\n",
      "Epoch: 50\n",
      "Batch Size: 24\n",
      "Optimizer learning rate: 1e-05\n",
      "Mean Squared Error (Perceptron): 18597.03026731106\n",
      "R^2: -8.522346\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: None\n",
      "Epoch: 50\n",
      "Batch Size: 24\n",
      "Optimizer learning rate: 0.0001\n",
      "Mean Squared Error (Perceptron): 7091.693555685888\n",
      "R^2: -2.6312008\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 462us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: None\n",
      "Epoch: 50\n",
      "Batch Size: 24\n",
      "Optimizer learning rate: 0.001\n",
      "Mean Squared Error (Perceptron): 15.203829560327568\n",
      "R^2: 0.9922151\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 446us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: None\n",
      "Epoch: 50\n",
      "Batch Size: 32\n",
      "Optimizer learning rate: 1e-05\n",
      "Mean Squared Error (Perceptron): 2181.375922285439\n",
      "R^2: -0.116942525\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 456us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: None\n",
      "Epoch: 50\n",
      "Batch Size: 32\n",
      "Optimizer learning rate: 0.0001\n",
      "Mean Squared Error (Perceptron): 2018.4709182730817\n",
      "R^2: -0.0335294\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 501us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: None\n",
      "Epoch: 50\n",
      "Batch Size: 32\n",
      "Optimizer learning rate: 0.001\n",
      "Mean Squared Error (Perceptron): 633.8809622617496\n",
      "R^2: 0.6754303\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 451us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: None\n",
      "Epoch: 50\n",
      "Batch Size: 40\n",
      "Optimizer learning rate: 1e-05\n",
      "Mean Squared Error (Perceptron): 1021.5777222010316\n",
      "R^2: 0.4769156\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 453us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: None\n",
      "Epoch: 50\n",
      "Batch Size: 40\n",
      "Optimizer learning rate: 0.0001\n",
      "Mean Squared Error (Perceptron): 4074.6541885495444\n",
      "R^2: -1.0863688\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 452us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: None\n",
      "Epoch: 50\n",
      "Batch Size: 40\n",
      "Optimizer learning rate: 0.001\n",
      "Mean Squared Error (Perceptron): 14525.606164672263\n",
      "R^2: -6.43763\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 464us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: None\n",
      "Epoch: 75\n",
      "Batch Size: 24\n",
      "Optimizer learning rate: 1e-05\n",
      "Mean Squared Error (Perceptron): 802.6369459675125\n",
      "R^2: 0.5890212\n",
      "----------------------------------------------------------------\n",
      "17/17 [==============================] - 0s 520us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: None\n",
      "Epoch: 75\n",
      "Batch Size: 24\n",
      "Optimizer learning rate: 0.0001\n",
      "Mean Squared Error (Perceptron): 246.44758710945828\n",
      "R^2: 0.87381005\n",
      "----------------------------------------------------------------\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/8k/4jdjx8mn6pvd9cxdzk57tgtc0000gn/T/ipykernel_31161/3256990813.py\", line 7, in <module>\n",
      "    mse, r2 = create_model_P(activation=activation, epochs=epochs, batch_size=batch_size, op_learning_rate=op_learning_rate)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/8k/4jdjx8mn6pvd9cxdzk57tgtc0000gn/T/ipykernel_31161/4117936375.py\", line 10, in create_model_P\n",
      "    history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, Y_test), verbose=0)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1832, in fit\n",
      "    val_logs = self.evaluate(\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2272, in evaluate\n",
      "    logs = test_function_runner.run_step(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 4079, in run_step\n",
      "    tmp_logs = self._function(dataset_or_iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 831, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 876, in _call\n",
      "    results = tracing_compilation.call_function(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 138, in call_function\n",
      "    flat_inputs = function.function_type.unpack_inputs(bound_args)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/tensorflow/core/function/polymorphism/function_type.py\", line 384, in unpack_inputs\n",
      "    p.type_constraint._to_tensors(bound_parameters.arguments[p.name])  # pylint: disable=protected-access\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/type_spec.py\", line 249, in _to_tensors\n",
      "    nest.map_structure(\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/nest.py\", line 629, in map_structure\n",
      "    return nest_util.map_structure(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py\", line 1168, in map_structure\n",
      "    return _tf_core_map_structure(func, *structure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py\", line 1206, in _tf_core_map_structure\n",
      "    return _tf_core_pack_sequence_as(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py\", line 1022, in _tf_core_pack_sequence_as\n",
      "    return sequence_fn(structure, packed)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py\", line 347, in sequence_like\n",
      "    elif isinstance(instance, CustomNestProtocol):\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/typing.py\", line 2003, in __instancecheck__\n",
      "    for attr in _get_protocol_attrs(cls)):\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/typing.py\", line -1, in _get_protocol_attrs\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/Users/kyleluo/anaconda3/lib/python3.11/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "ret = []\n",
    "for activation in [None, 'relu', 'sigmoid', 'tanh']:\n",
    "    for epochs in [50, 75, 100, 150]:\n",
    "        for batch_size in [24, 32, 40]:\n",
    "            for op_learning_rate in [1e-5, 1e-4, 1e-3]:\n",
    "                try:\n",
    "                    mse, r2 = create_model_P(activation=activation, epochs=epochs, batch_size=batch_size, op_learning_rate=op_learning_rate)\n",
    "                    ret.append([activation, epochs, batch_size, op_learning_rate, mse, r2])\n",
    "                except InvalidArgumentError as e:\n",
    "                    print(\"Model error with params\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c804f8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MSE for Perceptron: [None, 50, 24, 0.001, 15.203829560327568, 0.9922151]\n",
      "Best R^2 for Perceptron: [None, 50, 24, 0.001, 15.203829560327568, 0.9922151]\n"
     ]
    }
   ],
   "source": [
    "print(\"Best MSE for Perceptron:\", sorted(ret, key=lambda x:x[-2])[0])\n",
    "print(\"Best R^2 for Perceptron:\", sorted(ret, key=lambda x:-x[-1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a82d6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_D(activation='tanh', epochs=100, batch_size=32, op_learning_rate=1e-5, hidden=128, layers=11, dropout=0.1):\n",
    "    # Build a neural network model with 2 hidden layers\n",
    "    # You can experiment with different architectures, including the number of layers and neurons.\n",
    "    # Building DNN model with 11 hidden layers\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    # Input layer will have same number of neurons as number of feature variables\n",
    "    model.add(Dense(64, input_dim=X_train.shape[1], activation=activation))\n",
    "\n",
    "    # Hidden layers\n",
    "    # Play around with number of neurons in each hidden layer.\n",
    "    # Too many neurons leads to overcomplexity, not enough means too simple\n",
    "    # Tanh activation function here is used b/c it is recommended to use\n",
    "    # when there are more hidden layers.\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(hidden, activation=activation))\n",
    "        model.add(Dropout(dropout)) # This helps with preventing overfitting\n",
    "\n",
    "    # Output layer\n",
    "    # Output layer will have 1 neuron b/c there's only 1 target variable\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=op_learning_rate)  # Experiment with learning rate\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, Y_test), verbose=0)\n",
    "\n",
    "    # Make predictions\n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    # Inverse transform the scaled values\n",
    "    Y_test_original = scaler_Y.inverse_transform(Y_test)\n",
    "    Y_pred_original = scaler_Y.inverse_transform(Y_pred)\n",
    "\n",
    "    print(f\"Activation: {activation}\\nEpoch: {epochs}\\nBatch Size: {batch_size}\\nOptimizer learning rate: {op_learning_rate}\")\n",
    "    print(f\"Hidden: {hidden}\\nLayers: {layers}\\nDropout: {dropout}\")\n",
    "    # Calculate MSE and R2\n",
    "    mse = mean_squared_error(Y_test_original, Y_pred_original)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    metric = tf.keras.metrics.R2Score()\n",
    "    metric.update_state(Y_test_original, Y_pred_original)\n",
    "    r2 = metric.result().numpy()\n",
    "    print(\"R^2:\", metric.result().numpy())\n",
    "    print(\"-\"*64)\n",
    "    return mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064960be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "m, r = create_model_D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0ba123",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# GRID SEARCH\n",
    "\n",
    "ret = []\n",
    "for activation in ['relu', 'sigmoid', 'tanh']:\n",
    "    for epochs in [50, 75, 100, 150]:\n",
    "        for batch_size in [24, 32, 40]:\n",
    "            for op_learning_rate in [1e-5, 1e-4, 1e-3]:\n",
    "                for hidden in [32, 64, 128]:\n",
    "                    for layers in [6, 9, 11]:\n",
    "                        for dropout in [0.05, 0.1, 0.15]:\n",
    "                            try:\n",
    "                                mse, r2 = create_model_D(activation=activation, epochs=epochs, batch_size=batch_size, op_learning_rate=op_learning_rate, hidden=hidden, layers=layers, dropout=dropout)\n",
    "                                ret.append([activation, epochs, batch_size, op_learning_rate, hidden, layers, dropout, mse, r2])\n",
    "                            except:\n",
    "                                print(\"Model error with params\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ece162",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best MSE for DNN:\", sorted(ret, key=lambda x:x[-2])[0])\n",
    "print(\"Best R^2 for DNN:\", sorted(ret, key=lambda x:-x[-1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a78a21e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
